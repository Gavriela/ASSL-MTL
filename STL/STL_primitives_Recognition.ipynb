{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419960fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.viz_tools import *\n",
    "from pymo.preprocessing import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580bfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pickle \n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, BatchNormalization, \\\n",
    "    concatenate, Activation, dot, Lambda, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2598a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13500261999126513256\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6252920832\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 22547258003437934\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc738a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 40  # Replace with any fixed number\n",
    "\n",
    "# Set seed for reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc36238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f664919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Kneeling_Upright\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Standing_Ipose\n",
      "Upright\n",
      "Upright\n",
      "Upright\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Kneeling_ForwardBending\n",
      "Rotation\n",
      "Rotation\n",
      "Rotation\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Kneeling_Lateral_Bending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "Standing_ForwardBending\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "HandsAboveHead\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Kneeling_ForwardBendingArmExtention\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Standing_LateralBending\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Kneeling_StretchedArms\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Standing_StretchingWithArmExtension\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Kneeling_TorsoRotation\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_ArmsAboveHead_Extention\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Standing_Elbows90\n",
      "Train data 0 shape: (1567, 81)\n",
      "Train data 1 shape: (1567, 81)\n",
      "Train data 2 shape: (1567, 81)\n",
      "Train data 3 shape: (1567, 81)\n",
      "Train data 4 shape: (1567, 81)\n",
      "Train data 5 shape: (1567, 81)\n",
      "Train data 6 shape: (1567, 81)\n",
      "Train data 7 shape: (1567, 81)\n",
      "Train data 8 shape: (1567, 81)\n",
      "Train data 9 shape: (1567, 81)\n",
      "Train data 10 shape: (1567, 81)\n",
      "Train data 11 shape: (1567, 81)\n",
      "Train data 12 shape: (1567, 81)\n",
      "Train data 13 shape: (1567, 81)\n",
      "Train data 14 shape: (1567, 81)\n",
      "Train data 15 shape: (1567, 81)\n",
      "Train data 16 shape: (1567, 81)\n",
      "Train data 17 shape: (1567, 81)\n",
      "Train data 18 shape: (1567, 81)\n",
      "Train data 19 shape: (1567, 81)\n",
      "Train data 20 shape: (1567, 81)\n",
      "Train data 21 shape: (1567, 81)\n",
      "Train data 22 shape: (1567, 81)\n",
      "Train data 23 shape: (1567, 81)\n",
      "Train data 24 shape: (1567, 81)\n",
      "Train data 25 shape: (1567, 81)\n",
      "Train data 26 shape: (1567, 81)\n",
      "Train data 27 shape: (1567, 81)\n",
      "Train data 28 shape: (1567, 81)\n",
      "Train data 29 shape: (1567, 81)\n",
      "Train data 30 shape: (1567, 81)\n",
      "Train data 31 shape: (1567, 81)\n",
      "Train data 32 shape: (1567, 81)\n",
      "Train data 33 shape: (1567, 81)\n",
      "Train data 34 shape: (1567, 81)\n",
      "Train data 35 shape: (1567, 81)\n",
      "Train data 36 shape: (1567, 81)\n",
      "Train data 37 shape: (1567, 81)\n",
      "Train data 38 shape: (1567, 81)\n",
      "Train data 39 shape: (1567, 81)\n",
      "Train data 40 shape: (1567, 81)\n",
      "Train data 41 shape: (1567, 81)\n",
      "Train data 42 shape: (1567, 81)\n",
      "Train data 43 shape: (1567, 81)\n",
      "Train data 44 shape: (1567, 81)\n",
      "Train data 45 shape: (1567, 81)\n",
      "Train data 46 shape: (1567, 81)\n",
      "Train data 47 shape: (1567, 81)\n",
      "Train data 48 shape: (1567, 81)\n",
      "Train data 49 shape: (1567, 81)\n",
      "Train data 50 shape: (1567, 81)\n",
      "Train data 51 shape: (1567, 81)\n",
      "Train data 52 shape: (1567, 81)\n",
      "Train data 53 shape: (1567, 81)\n",
      "Train data 54 shape: (1567, 81)\n",
      "Train data 55 shape: (1567, 81)\n",
      "Train data 56 shape: (1567, 81)\n",
      "Train data 57 shape: (1567, 81)\n",
      "Train data 58 shape: (1567, 81)\n",
      "Train data 59 shape: (1567, 81)\n",
      "Train data 60 shape: (1567, 81)\n",
      "Train data 61 shape: (1567, 81)\n",
      "Train data 62 shape: (1567, 81)\n",
      "Train data 63 shape: (1567, 81)\n",
      "Train data 64 shape: (1567, 81)\n",
      "Train data 65 shape: (1567, 81)\n",
      "Train data 66 shape: (1567, 81)\n",
      "Train data 67 shape: (1567, 81)\n",
      "Train data 68 shape: (1567, 81)\n",
      "Train data 69 shape: (1567, 81)\n",
      "Train data 70 shape: (1567, 81)\n",
      "Train data 71 shape: (1567, 81)\n",
      "Train data 72 shape: (1567, 81)\n",
      "Train data 73 shape: (1567, 81)\n",
      "Train data 74 shape: (1567, 81)\n",
      "Train data 75 shape: (1567, 81)\n",
      "Train data 76 shape: (1567, 81)\n",
      "Train data 77 shape: (1567, 81)\n",
      "Train data 78 shape: (1567, 81)\n",
      "Train data 79 shape: (1567, 81)\n",
      "Train data 80 shape: (1567, 81)\n",
      "Train data 81 shape: (1567, 81)\n",
      "Train data 82 shape: (1567, 81)\n",
      "Train data 83 shape: (1567, 81)\n",
      "Train data 84 shape: (1567, 81)\n",
      "Train data 85 shape: (1567, 81)\n",
      "Train data 86 shape: (1567, 81)\n",
      "Train data 87 shape: (1567, 81)\n",
      "Train data 88 shape: (1567, 81)\n",
      "Train data 89 shape: (1567, 81)\n",
      "Train data 90 shape: (1567, 81)\n",
      "Train data 91 shape: (1567, 81)\n",
      "Train data 92 shape: (1567, 81)\n",
      "Train data 93 shape: (1567, 81)\n",
      "Train data 94 shape: (1567, 81)\n",
      "Train data 95 shape: (1567, 81)\n",
      "Train data 96 shape: (1567, 81)\n",
      "Train data 97 shape: (1567, 81)\n",
      "Train data 98 shape: (1567, 81)\n",
      "Train data 99 shape: (1567, 81)\n",
      "Train data 100 shape: (1567, 81)\n",
      "Train data 101 shape: (1567, 81)\n",
      "Train data 102 shape: (1567, 81)\n",
      "Train data 103 shape: (1567, 81)\n",
      "Train data 104 shape: (1567, 81)\n",
      "Train data 105 shape: (1567, 81)\n",
      "Train data 106 shape: (1567, 81)\n",
      "Train data 107 shape: (1567, 81)\n",
      "Train data 108 shape: (1567, 81)\n",
      "Train data 109 shape: (1567, 81)\n",
      "Train data 110 shape: (1567, 81)\n",
      "Train data 111 shape: (1567, 81)\n",
      "Train data 112 shape: (1567, 81)\n",
      "Train data 113 shape: (1567, 81)\n",
      "Train data 114 shape: (1567, 81)\n",
      "Train data 115 shape: (1567, 81)\n",
      "Train data 116 shape: (1567, 81)\n",
      "Train data 117 shape: (1567, 81)\n",
      "Train data 118 shape: (1567, 81)\n",
      "Train data 119 shape: (1567, 81)\n",
      "Train data 120 shape: (1567, 81)\n",
      "Train data 121 shape: (1567, 81)\n",
      "Train data 122 shape: (1567, 81)\n",
      "Train data 123 shape: (1567, 81)\n",
      "Train data 124 shape: (1567, 81)\n",
      "Train data 125 shape: (1567, 81)\n",
      "Train data 126 shape: (1567, 81)\n",
      "Train data 127 shape: (1567, 81)\n",
      "Train data 128 shape: (1567, 81)\n",
      "Train data 129 shape: (1567, 81)\n",
      "Train data 130 shape: (1567, 81)\n",
      "Train data 131 shape: (1567, 81)\n",
      "Train data 132 shape: (1567, 81)\n",
      "Train data 133 shape: (1567, 81)\n",
      "Train data 134 shape: (1567, 81)\n",
      "Train data 135 shape: (1567, 81)\n",
      "Train data 136 shape: (1567, 81)\n",
      "Train data 137 shape: (1567, 81)\n",
      "Train data 138 shape: (1567, 81)\n",
      "Train data 139 shape: (1567, 81)\n",
      "Train data 140 shape: (1567, 81)\n",
      "Train data 141 shape: (1567, 81)\n",
      "Train data 142 shape: (1567, 81)\n",
      "Train data 143 shape: (1567, 81)\n",
      "Train data 144 shape: (1567, 81)\n",
      "Train data 145 shape: (1567, 81)\n",
      "Train data 146 shape: (1567, 81)\n",
      "Train data 147 shape: (1567, 81)\n",
      "Train data 148 shape: (1567, 81)\n",
      "Train data 149 shape: (1567, 81)\n",
      "Train data 150 shape: (1567, 81)\n",
      "Train data 151 shape: (1567, 81)\n",
      "Train data 152 shape: (1567, 81)\n",
      "Train data 153 shape: (1567, 81)\n",
      "Train data 154 shape: (1567, 81)\n",
      "Train data 155 shape: (1567, 81)\n",
      "Train data 156 shape: (1567, 81)\n",
      "Train data 157 shape: (1567, 81)\n",
      "Train data 158 shape: (1567, 81)\n",
      "Train data 159 shape: (1567, 81)\n",
      "Train data 160 shape: (1567, 81)\n",
      "Train data 161 shape: (1567, 81)\n",
      "Train data 162 shape: (1567, 81)\n",
      "Train data 163 shape: (1567, 81)\n",
      "Train data 164 shape: (1567, 81)\n",
      "Train data 165 shape: (1567, 81)\n",
      "Train data 166 shape: (1567, 81)\n",
      "Train data 167 shape: (1567, 81)\n",
      "Train data 168 shape: (1567, 81)\n",
      "Train data 169 shape: (1567, 81)\n",
      "Train data 170 shape: (1567, 81)\n",
      "Train data 171 shape: (1567, 81)\n",
      "Train data 172 shape: (1567, 81)\n",
      "Train data 173 shape: (1567, 81)\n",
      "Train data 174 shape: (1567, 81)\n",
      "Train data 175 shape: (1567, 81)\n",
      "Train data 176 shape: (1567, 81)\n",
      "Train data 177 shape: (1567, 81)\n",
      "Train data 178 shape: (1567, 81)\n",
      "Train data 179 shape: (1567, 81)\n",
      "Train data 180 shape: (1567, 81)\n",
      "Train data 181 shape: (1567, 81)\n",
      "Train data 182 shape: (1567, 81)\n",
      "Train data 183 shape: (1567, 81)\n",
      "Train data 184 shape: (1567, 81)\n",
      "Train data 185 shape: (1567, 81)\n",
      "Train data 186 shape: (1567, 81)\n",
      "Train data 187 shape: (1567, 81)\n",
      "Train data 188 shape: (1567, 81)\n",
      "Train data 189 shape: (1567, 81)\n",
      "Train data 190 shape: (1567, 81)\n",
      "Train data 191 shape: (1567, 81)\n",
      "Train data 192 shape: (1567, 81)\n",
      "Train data 193 shape: (1567, 81)\n",
      "Train data 194 shape: (1567, 81)\n",
      "Train data 195 shape: (1567, 81)\n",
      "Train data 196 shape: (1567, 81)\n",
      "Train data 197 shape: (1567, 81)\n",
      "Train data 198 shape: (1567, 81)\n",
      "Train data 199 shape: (1567, 81)\n",
      "Train data 200 shape: (1567, 81)\n",
      "Train data 201 shape: (1567, 81)\n",
      "Train data 202 shape: (1567, 81)\n",
      "Train data 203 shape: (1567, 81)\n",
      "Train data 204 shape: (1567, 81)\n",
      "Train data 205 shape: (1567, 81)\n",
      "Train data 206 shape: (1567, 81)\n",
      "Train data 207 shape: (1567, 81)\n",
      "Train data 208 shape: (1567, 81)\n",
      "Train data 209 shape: (1567, 81)\n",
      "Train data 210 shape: (1567, 81)\n",
      "Train data 211 shape: (1567, 81)\n",
      "Train data 212 shape: (1567, 81)\n",
      "Train data 213 shape: (1567, 81)\n",
      "Train data 214 shape: (1567, 81)\n",
      "Train data 215 shape: (1567, 81)\n",
      "Train data 216 shape: (1567, 81)\n",
      "Train data 217 shape: (1567, 81)\n",
      "Train data 218 shape: (1567, 81)\n",
      "Train data 219 shape: (1567, 81)\n",
      "Train data 220 shape: (1567, 81)\n",
      "Train data 221 shape: (1567, 81)\n",
      "Train data 222 shape: (1567, 81)\n",
      "Train data 223 shape: (1567, 81)\n",
      "Train data 224 shape: (1567, 81)\n",
      "Train data 225 shape: (1567, 81)\n",
      "Train data 226 shape: (1567, 81)\n",
      "Train data 227 shape: (1567, 81)\n",
      "Train data 228 shape: (1567, 81)\n",
      "Train data 229 shape: (1567, 81)\n",
      "Train data 230 shape: (1567, 81)\n",
      "Train data 231 shape: (1567, 81)\n",
      "Train data 232 shape: (1567, 81)\n",
      "Train data 233 shape: (1567, 81)\n",
      "Train data 234 shape: (1567, 81)\n",
      "Train data 235 shape: (1567, 81)\n",
      "Train data 236 shape: (1567, 81)\n",
      "Train data 237 shape: (1567, 81)\n",
      "Train data 238 shape: (1567, 81)\n",
      "Train data 239 shape: (1567, 81)\n",
      "Train data 240 shape: (1567, 81)\n",
      "Train data 241 shape: (1567, 81)\n",
      "Train data 242 shape: (1567, 81)\n",
      "Train data 243 shape: (1567, 81)\n",
      "Train data 244 shape: (1567, 81)\n",
      "Train data 245 shape: (1567, 81)\n",
      "Train data 246 shape: (1567, 81)\n",
      "Train data 247 shape: (1567, 81)\n",
      "Train data 248 shape: (1567, 81)\n",
      "Train data 249 shape: (1567, 81)\n",
      "Train data 250 shape: (1567, 81)\n",
      "Train data 251 shape: (1567, 81)\n",
      "Train data 252 shape: (1567, 81)\n",
      "Train data 253 shape: (1567, 81)\n",
      "Train data 254 shape: (1567, 81)\n",
      "Train data 255 shape: (1567, 81)\n",
      "Train data 256 shape: (1567, 81)\n",
      "Train data 257 shape: (1567, 81)\n",
      "Train data 258 shape: (1567, 81)\n",
      "Train data 259 shape: (1567, 81)\n",
      "Train data 260 shape: (1567, 81)\n",
      "Train data 261 shape: (1567, 81)\n",
      "Train data 262 shape: (1567, 81)\n",
      "Train data 263 shape: (1567, 81)\n",
      "Train data 264 shape: (1567, 81)\n",
      "Train data 265 shape: (1567, 81)\n",
      "Train data 266 shape: (1567, 81)\n",
      "Train data 267 shape: (1567, 81)\n",
      "Train data 268 shape: (1567, 81)\n",
      "Train data 269 shape: (1567, 81)\n",
      "Train data 270 shape: (1567, 81)\n",
      "Train data 271 shape: (1567, 81)\n",
      "Train data 272 shape: (1567, 81)\n",
      "Train data 273 shape: (1567, 81)\n",
      "Train data 274 shape: (1567, 81)\n",
      "Train data 275 shape: (1567, 81)\n",
      "Train data 276 shape: (1567, 81)\n",
      "Train data 277 shape: (1567, 81)\n",
      "Train data 278 shape: (1567, 81)\n",
      "Train data 279 shape: (1567, 81)\n",
      "Train data 280 shape: (1567, 81)\n",
      "Train data 281 shape: (1567, 81)\n",
      "Train data 282 shape: (1567, 81)\n",
      "Train data 283 shape: (1567, 81)\n",
      "Train data 284 shape: (1567, 81)\n",
      "Train data 285 shape: (1567, 81)\n",
      "Train data 286 shape: (1567, 81)\n",
      "Train data 287 shape: (1567, 81)\n",
      "Train data 288 shape: (1567, 81)\n",
      "Train data 289 shape: (1567, 81)\n",
      "Train data 290 shape: (1567, 81)\n",
      "Train data 291 shape: (1567, 81)\n",
      "Train data 292 shape: (1567, 81)\n",
      "Train data 293 shape: (1567, 81)\n",
      "Train data 294 shape: (1567, 81)\n",
      "Train data 295 shape: (1567, 81)\n",
      "Train data 296 shape: (1567, 81)\n",
      "Train data 297 shape: (1567, 81)\n",
      "Train data 298 shape: (1567, 81)\n",
      "Train data 299 shape: (1567, 81)\n",
      "Train data 300 shape: (1567, 81)\n",
      "Train data 301 shape: (1567, 81)\n",
      "Train data 302 shape: (1567, 81)\n",
      "Train data 303 shape: (1567, 81)\n",
      "Train data 304 shape: (1567, 81)\n",
      "Train data 305 shape: (1567, 81)\n",
      "Train data 306 shape: (1567, 81)\n",
      "Train data 307 shape: (1567, 81)\n",
      "Train data 308 shape: (1567, 81)\n",
      "Train data 309 shape: (1567, 81)\n",
      "Train data 310 shape: (1567, 81)\n",
      "Train data 311 shape: (1567, 81)\n",
      "Train data 312 shape: (1567, 81)\n",
      "Train data 313 shape: (1567, 81)\n",
      "Train data 314 shape: (1567, 81)\n",
      "Train data 315 shape: (1567, 81)\n",
      "Train data 316 shape: (1567, 81)\n",
      "Train data 317 shape: (1567, 81)\n",
      "Train data 318 shape: (1567, 81)\n",
      "Train data 319 shape: (1567, 81)\n",
      "Train data 320 shape: (1567, 81)\n",
      "Train data 321 shape: (1567, 81)\n",
      "Train data 322 shape: (1567, 81)\n",
      "Train data 323 shape: (1567, 81)\n",
      "Train data 324 shape: (1567, 81)\n",
      "Train data 325 shape: (1567, 81)\n",
      "Train data 326 shape: (1567, 81)\n",
      "Train data 327 shape: (1567, 81)\n",
      "Train data 328 shape: (1567, 81)\n",
      "Train data 329 shape: (1567, 81)\n",
      "Train data 330 shape: (1567, 81)\n",
      "Train data 331 shape: (1567, 81)\n",
      "Train data 332 shape: (1567, 81)\n",
      "Train data 333 shape: (1567, 81)\n",
      "Train data 334 shape: (1567, 81)\n",
      "Train data 335 shape: (1567, 81)\n",
      "Train data 336 shape: (1567, 81)\n",
      "Train data 337 shape: (1567, 81)\n",
      "Train data 338 shape: (1567, 81)\n",
      "Train data 339 shape: (1567, 81)\n",
      "Train data 340 shape: (1567, 81)\n",
      "Train data 341 shape: (1567, 81)\n",
      "Train data 342 shape: (1567, 81)\n",
      "Train data 343 shape: (1567, 81)\n",
      "Train data 344 shape: (1567, 81)\n",
      "Train data 345 shape: (1567, 81)\n",
      "Train data 346 shape: (1567, 81)\n",
      "Train data 347 shape: (1567, 81)\n",
      "Train data 348 shape: (1567, 81)\n",
      "Train data 349 shape: (1567, 81)\n",
      "Train data 350 shape: (1567, 81)\n",
      "Train data 351 shape: (1567, 81)\n",
      "Train data 352 shape: (1567, 81)\n",
      "Train data 353 shape: (1567, 81)\n",
      "Train data 354 shape: (1567, 81)\n",
      "Train data 355 shape: (1567, 81)\n",
      "Train data 356 shape: (1567, 81)\n",
      "Train data 357 shape: (1567, 81)\n",
      "Train data 358 shape: (1567, 81)\n",
      "Train data 359 shape: (1567, 81)\n",
      "Train data 360 shape: (1567, 81)\n",
      "Train data 361 shape: (1567, 81)\n",
      "Train data 362 shape: (1567, 81)\n",
      "Train data 363 shape: (1567, 81)\n",
      "Train data 364 shape: (1567, 81)\n",
      "Train data 365 shape: (1567, 81)\n",
      "Train data 366 shape: (1567, 81)\n",
      "Train data 367 shape: (1567, 81)\n",
      "Train data 368 shape: (1567, 81)\n",
      "Train data 369 shape: (1567, 81)\n",
      "Train data 370 shape: (1567, 81)\n",
      "Train data 371 shape: (1567, 81)\n",
      "Train data 372 shape: (1567, 81)\n",
      "Train data 373 shape: (1567, 81)\n",
      "Train data 374 shape: (1567, 81)\n",
      "Train data 375 shape: (1567, 81)\n",
      "Train data 376 shape: (1567, 81)\n",
      "Train data 377 shape: (1567, 81)\n",
      "Train data 378 shape: (1567, 81)\n",
      "Train data 379 shape: (1567, 81)\n",
      "Train data 380 shape: (1567, 81)\n",
      "Train data 381 shape: (1567, 81)\n",
      "Train data 382 shape: (1567, 81)\n",
      "Train data 383 shape: (1567, 81)\n",
      "Train data 384 shape: (1567, 81)\n",
      "Train data 385 shape: (1567, 81)\n",
      "Train data 386 shape: (1567, 81)\n",
      "Train data 387 shape: (1567, 81)\n",
      "Train data 388 shape: (1567, 81)\n",
      "Train data 389 shape: (1567, 81)\n",
      "Train data 390 shape: (1567, 81)\n",
      "Train data 391 shape: (1567, 81)\n",
      "Train data 392 shape: (1567, 81)\n",
      "Train data 393 shape: (1567, 81)\n",
      "Train data 394 shape: (1567, 81)\n",
      "Train data 395 shape: (1567, 81)\n",
      "Train data 396 shape: (1567, 81)\n",
      "Train data 397 shape: (1567, 81)\n",
      "Train data 398 shape: (1567, 81)\n",
      "Train data 399 shape: (1567, 81)\n",
      "Train data 400 shape: (1567, 81)\n",
      "Train data 401 shape: (1567, 81)\n",
      "Train data 402 shape: (1567, 81)\n",
      "Train data 403 shape: (1567, 81)\n",
      "Train data 404 shape: (1567, 81)\n",
      "Train data 405 shape: (1567, 81)\n",
      "Train data 406 shape: (1567, 81)\n",
      "Train data 407 shape: (1567, 81)\n",
      "Train data 408 shape: (1567, 81)\n",
      "Train data 409 shape: (1567, 81)\n",
      "Train data 410 shape: (1567, 81)\n",
      "Train data 411 shape: (1567, 81)\n",
      "Train data 412 shape: (1567, 81)\n",
      "Train data 413 shape: (1567, 81)\n",
      "Train data 414 shape: (1567, 81)\n",
      "Train data 415 shape: (1567, 81)\n",
      "Train data 416 shape: (1567, 81)\n",
      "Train data 417 shape: (1567, 81)\n",
      "Train data 418 shape: (1567, 81)\n",
      "Train data 419 shape: (1567, 81)\n",
      "Train data 420 shape: (1567, 81)\n",
      "Train data 421 shape: (1567, 81)\n",
      "Train data 422 shape: (1567, 81)\n",
      "Train data 423 shape: (1567, 81)\n",
      "Train data 424 shape: (1567, 81)\n",
      "Train data 425 shape: (1567, 81)\n",
      "Train data 426 shape: (1567, 81)\n",
      "Train data 427 shape: (1567, 81)\n",
      "Train data 428 shape: (1567, 81)\n",
      "Train data 429 shape: (1567, 81)\n",
      "Train data 430 shape: (1567, 81)\n",
      "Train data 431 shape: (1567, 81)\n",
      "Train data 432 shape: (1567, 81)\n",
      "Train data 433 shape: (1567, 81)\n",
      "Train data 434 shape: (1567, 81)\n",
      "Train data 435 shape: (1567, 81)\n",
      "Train data 436 shape: (1567, 81)\n",
      "Train data 437 shape: (1567, 81)\n",
      "Train data 438 shape: (1567, 81)\n",
      "Train data 439 shape: (1567, 81)\n",
      "Train data 440 shape: (1567, 81)\n",
      "Train data 441 shape: (1567, 81)\n",
      "Train data 442 shape: (1567, 81)\n",
      "Train data 443 shape: (1567, 81)\n",
      "Train data 444 shape: (1567, 81)\n",
      "Train data 445 shape: (1567, 81)\n",
      "Train data 446 shape: (1567, 81)\n",
      "Train data 447 shape: (1567, 81)\n",
      "Train data 448 shape: (1567, 81)\n",
      "Train data 449 shape: (1567, 81)\n",
      "Train data 450 shape: (1567, 81)\n",
      "Train data 451 shape: (1567, 81)\n",
      "Train data 452 shape: (1567, 81)\n",
      "Train data 453 shape: (1567, 81)\n",
      "Train data 454 shape: (1567, 81)\n",
      "Train data 455 shape: (1567, 81)\n",
      "Train data 456 shape: (1567, 81)\n",
      "Train data 457 shape: (1567, 81)\n",
      "Train data 458 shape: (1567, 81)\n",
      "Train data 459 shape: (1567, 81)\n",
      "Train data 460 shape: (1567, 81)\n",
      "Train data 461 shape: (1567, 81)\n",
      "Train data 462 shape: (1567, 81)\n",
      "Train data 463 shape: (1567, 81)\n",
      "Train data 464 shape: (1567, 81)\n",
      "Train data 465 shape: (1567, 81)\n",
      "Train data 466 shape: (1567, 81)\n",
      "Train data 467 shape: (1567, 81)\n",
      "Train data 468 shape: (1567, 81)\n",
      "Train data 469 shape: (1567, 81)\n",
      "Train data 470 shape: (1567, 81)\n",
      "Train data 471 shape: (1567, 81)\n",
      "Train data 472 shape: (1567, 81)\n",
      "Train data 473 shape: (1567, 81)\n",
      "Train data 474 shape: (1567, 81)\n",
      "Train data 475 shape: (1567, 81)\n",
      "Train data 476 shape: (1567, 81)\n",
      "Train data 477 shape: (1567, 81)\n",
      "Train data 478 shape: (1567, 81)\n",
      "Train data 479 shape: (1567, 81)\n",
      "Train data 480 shape: (1567, 81)\n",
      "Train data 481 shape: (1567, 81)\n",
      "Train data 482 shape: (1567, 81)\n",
      "Train data 483 shape: (1567, 81)\n",
      "Train data 484 shape: (1567, 81)\n",
      "Train data 485 shape: (1567, 81)\n",
      "Train data 486 shape: (1567, 81)\n",
      "Train data 487 shape: (1567, 81)\n",
      "Train data 488 shape: (1567, 81)\n",
      "Train data 489 shape: (1567, 81)\n",
      "Train data 490 shape: (1567, 81)\n",
      "Train data 491 shape: (1567, 81)\n",
      "Train data 492 shape: (1567, 81)\n",
      "Train data 493 shape: (1567, 81)\n",
      "Train data 494 shape: (1567, 81)\n",
      "Train data 495 shape: (1567, 81)\n",
      "Train data 496 shape: (1567, 81)\n",
      "Train data 497 shape: (1567, 81)\n",
      "Train data 498 shape: (1567, 81)\n",
      "Train data 499 shape: (1567, 81)\n",
      "Train data 500 shape: (1567, 81)\n",
      "Train data 501 shape: (1567, 81)\n",
      "Train data 502 shape: (1567, 81)\n",
      "Train data 503 shape: (1567, 81)\n",
      "Train data 504 shape: (1567, 81)\n",
      "Train data 505 shape: (1567, 81)\n",
      "Train data 506 shape: (1567, 81)\n",
      "Train data 507 shape: (1567, 81)\n",
      "Train data 508 shape: (1567, 81)\n",
      "Train data 509 shape: (1567, 81)\n",
      "Train data 510 shape: (1567, 81)\n",
      "Train data 511 shape: (1567, 81)\n",
      "Train data 512 shape: (1567, 81)\n",
      "Train data 513 shape: (1567, 81)\n",
      "Train data 514 shape: (1567, 81)\n",
      "Train data 515 shape: (1567, 81)\n",
      "Train data 516 shape: (1567, 81)\n",
      "Train data 517 shape: (1567, 81)\n",
      "Train data 518 shape: (1567, 81)\n",
      "Train data 519 shape: (1567, 81)\n",
      "Train data 520 shape: (1567, 81)\n",
      "Train data 521 shape: (1567, 81)\n",
      "Train data 522 shape: (1567, 81)\n",
      "Train data 523 shape: (1567, 81)\n",
      "Train data 524 shape: (1567, 81)\n",
      "Train data 525 shape: (1567, 81)\n",
      "Train data 526 shape: (1567, 81)\n",
      "Train data 527 shape: (1567, 81)\n",
      "Train data 528 shape: (1567, 81)\n",
      "Train data 529 shape: (1567, 81)\n",
      "Train data 530 shape: (1567, 81)\n",
      "Train data 531 shape: (1567, 81)\n",
      "Train data 532 shape: (1567, 81)\n",
      "Train data 533 shape: (1567, 81)\n",
      "Train data 534 shape: (1567, 81)\n",
      "Train data 535 shape: (1567, 81)\n",
      "Train data 536 shape: (1567, 81)\n",
      "Train data 537 shape: (1567, 81)\n",
      "Train data 538 shape: (1567, 81)\n",
      "Train data 539 shape: (1567, 81)\n",
      "Train data 540 shape: (1567, 81)\n",
      "Train data 541 shape: (1567, 81)\n",
      "Train data 542 shape: (1567, 81)\n",
      "Train data 543 shape: (1567, 81)\n",
      "Train data 544 shape: (1567, 81)\n",
      "Train data 545 shape: (1567, 81)\n",
      "Train data 546 shape: (1567, 81)\n",
      "Train data 547 shape: (1567, 81)\n",
      "Train data 548 shape: (1567, 81)\n",
      "Train data 549 shape: (1567, 81)\n",
      "Train data 550 shape: (1567, 81)\n",
      "Train data 551 shape: (1567, 81)\n",
      "Train data 552 shape: (1567, 81)\n",
      "Train data 553 shape: (1567, 81)\n",
      "Train data 554 shape: (1567, 81)\n",
      "Train data 555 shape: (1567, 81)\n",
      "Train data 556 shape: (1567, 81)\n",
      "Train data 557 shape: (1567, 81)\n",
      "Train data 558 shape: (1567, 81)\n",
      "Train data 559 shape: (1567, 81)\n",
      "Train data 560 shape: (1567, 81)\n",
      "Train data 561 shape: (1567, 81)\n",
      "Train data 562 shape: (1567, 81)\n",
      "Train data 563 shape: (1567, 81)\n",
      "Train data 564 shape: (1567, 81)\n",
      "Train data 565 shape: (1567, 81)\n",
      "Train data 566 shape: (1567, 81)\n",
      "Train data 567 shape: (1567, 81)\n",
      "Train data 568 shape: (1567, 81)\n",
      "Train data 569 shape: (1567, 81)\n",
      "Train data 570 shape: (1567, 81)\n",
      "Train data 571 shape: (1567, 81)\n",
      "Train data 572 shape: (1567, 81)\n",
      "Train data 573 shape: (1567, 81)\n",
      "Train data 574 shape: (1567, 81)\n",
      "Train data 575 shape: (1567, 81)\n",
      "Train data 576 shape: (1567, 81)\n",
      "Train data 577 shape: (1567, 81)\n",
      "Train data 578 shape: (1567, 81)\n",
      "Train data 579 shape: (1567, 81)\n",
      "Train data 580 shape: (1567, 81)\n",
      "Train data 581 shape: (1567, 81)\n",
      "Train data 582 shape: (1567, 81)\n",
      "Train data 583 shape: (1567, 81)\n",
      "Train data 584 shape: (1567, 81)\n",
      "Train data 585 shape: (1567, 81)\n",
      "Train data 586 shape: (1567, 81)\n",
      "Train data 587 shape: (1567, 81)\n",
      "Train data 588 shape: (1567, 81)\n",
      "Train data 589 shape: (1567, 81)\n",
      "Train data 590 shape: (1567, 81)\n",
      "Train data 591 shape: (1567, 81)\n",
      "Train data 592 shape: (1567, 81)\n",
      "Train data 593 shape: (1567, 81)\n",
      "Train data 594 shape: (1567, 81)\n",
      "Train data 595 shape: (1567, 81)\n",
      "Train data 596 shape: (1567, 81)\n",
      "Train data 597 shape: (1567, 81)\n",
      "Train data 598 shape: (1567, 81)\n",
      "Train data 599 shape: (1567, 81)\n",
      "Train data 600 shape: (1567, 81)\n",
      "Train data 601 shape: (1567, 81)\n",
      "Train data 602 shape: (1567, 81)\n",
      "Train data 603 shape: (1567, 81)\n",
      "Train data 604 shape: (1567, 81)\n",
      "Train data 605 shape: (1567, 81)\n",
      "Train data 606 shape: (1567, 81)\n",
      "Train data 607 shape: (1567, 81)\n",
      "Train data 608 shape: (1567, 81)\n",
      "Train data 609 shape: (1567, 81)\n",
      "Train data 610 shape: (1567, 81)\n",
      "Train data 611 shape: (1567, 81)\n",
      "Train data 612 shape: (1567, 81)\n",
      "Train data 613 shape: (1567, 81)\n",
      "Train data 614 shape: (1567, 81)\n",
      "Train data 615 shape: (1567, 81)\n",
      "Train data 616 shape: (1567, 81)\n",
      "Train data 617 shape: (1567, 81)\n",
      "Train data 618 shape: (1567, 81)\n",
      "Train data 619 shape: (1567, 81)\n",
      "Train data 620 shape: (1567, 81)\n",
      "Train data 621 shape: (1567, 81)\n",
      "Train data 622 shape: (1567, 81)\n",
      "Train data 623 shape: (1567, 81)\n",
      "Train data 624 shape: (1567, 81)\n",
      "Train data 625 shape: (1567, 81)\n",
      "Train data 626 shape: (1567, 81)\n",
      "Train data 627 shape: (1567, 81)\n",
      "Train data 628 shape: (1567, 81)\n",
      "Train data 629 shape: (1567, 81)\n",
      "Train data 630 shape: (1567, 81)\n",
      "Train data 631 shape: (1567, 81)\n",
      "Train data 632 shape: (1567, 81)\n",
      "Train data 633 shape: (1567, 81)\n",
      "Train data 634 shape: (1567, 81)\n",
      "Train data 635 shape: (1567, 81)\n",
      "Train data 636 shape: (1567, 81)\n",
      "Train data 637 shape: (1567, 81)\n",
      "Train data 638 shape: (1567, 81)\n",
      "Train data 639 shape: (1567, 81)\n",
      "Train data 640 shape: (1567, 81)\n",
      "Train data 641 shape: (1567, 81)\n",
      "Train data 642 shape: (1567, 81)\n",
      "Train data 643 shape: (1567, 81)\n",
      "Train data 644 shape: (1567, 81)\n",
      "Train data 645 shape: (1567, 81)\n",
      "Train data 646 shape: (1567, 81)\n",
      "Train data 647 shape: (1567, 81)\n",
      "Train data 648 shape: (1567, 81)\n",
      "Train data 649 shape: (1567, 81)\n",
      "Train data 650 shape: (1567, 81)\n",
      "Train data 651 shape: (1567, 81)\n",
      "Train data 652 shape: (1567, 81)\n",
      "Train data 653 shape: (1567, 81)\n",
      "Train data 654 shape: (1567, 81)\n",
      "Train data 655 shape: (1567, 81)\n",
      "Train data 656 shape: (1567, 81)\n",
      "Train data 657 shape: (1567, 81)\n",
      "Train data 658 shape: (1567, 81)\n",
      "Train data 659 shape: (1567, 81)\n",
      "Train data 660 shape: (1567, 81)\n",
      "Train data 661 shape: (1567, 81)\n",
      "Train data 662 shape: (1567, 81)\n",
      "Train data 663 shape: (1567, 81)\n",
      "Train data 664 shape: (1567, 81)\n",
      "Train data 665 shape: (1567, 81)\n",
      "Train data 666 shape: (1567, 81)\n",
      "Train data 667 shape: (1567, 81)\n",
      "Train data 668 shape: (1567, 81)\n",
      "Train data 669 shape: (1567, 81)\n",
      "Train data 670 shape: (1567, 81)\n",
      "Train data 671 shape: (1567, 81)\n",
      "Train data 672 shape: (1567, 81)\n",
      "Test data 0 shape: (1567, 81)\n",
      "Test data 1 shape: (1567, 81)\n",
      "Test data 2 shape: (1567, 81)\n",
      "Test data 3 shape: (1567, 81)\n",
      "Test data 4 shape: (1567, 81)\n",
      "Test data 5 shape: (1567, 81)\n",
      "Test data 6 shape: (1567, 81)\n",
      "Test data 7 shape: (1567, 81)\n",
      "Test data 8 shape: (1567, 81)\n",
      "Test data 9 shape: (1567, 81)\n",
      "Test data 10 shape: (1567, 81)\n",
      "Test data 11 shape: (1567, 81)\n",
      "Test data 12 shape: (1567, 81)\n",
      "Test data 13 shape: (1567, 81)\n",
      "Test data 14 shape: (1567, 81)\n",
      "Test data 15 shape: (1567, 81)\n",
      "Test data 16 shape: (1567, 81)\n",
      "Test data 17 shape: (1567, 81)\n",
      "Test data 18 shape: (1567, 81)\n",
      "Test data 19 shape: (1567, 81)\n",
      "Test data 20 shape: (1567, 81)\n",
      "Test data 21 shape: (1567, 81)\n",
      "Test data 22 shape: (1567, 81)\n",
      "Test data 23 shape: (1567, 81)\n",
      "Test data 24 shape: (1567, 81)\n",
      "Test data 25 shape: (1567, 81)\n",
      "Test data 26 shape: (1567, 81)\n",
      "Test data 27 shape: (1567, 81)\n",
      "Test data 28 shape: (1567, 81)\n",
      "Test data 29 shape: (1567, 81)\n",
      "Test data 30 shape: (1567, 81)\n",
      "Test data 31 shape: (1567, 81)\n",
      "Test data 32 shape: (1567, 81)\n",
      "Test data 33 shape: (1567, 81)\n",
      "Test data 34 shape: (1567, 81)\n",
      "Test data 35 shape: (1567, 81)\n",
      "Test data 36 shape: (1567, 81)\n",
      "Test data 37 shape: (1567, 81)\n",
      "Test data 38 shape: (1567, 81)\n",
      "Test data 39 shape: (1567, 81)\n",
      "Test data 40 shape: (1567, 81)\n",
      "Test data 41 shape: (1567, 81)\n",
      "Test data 42 shape: (1567, 81)\n",
      "Test data 43 shape: (1567, 81)\n",
      "Test data 44 shape: (1567, 81)\n",
      "Test data 45 shape: (1567, 81)\n",
      "Test data 46 shape: (1567, 81)\n",
      "Test data 47 shape: (1567, 81)\n",
      "Test data 48 shape: (1567, 81)\n",
      "Test data 49 shape: (1567, 81)\n",
      "Test data 50 shape: (1567, 81)\n",
      "Test data 51 shape: (1567, 81)\n",
      "Test data 52 shape: (1567, 81)\n",
      "Test data 53 shape: (1567, 81)\n",
      "Test data 54 shape: (1567, 81)\n",
      "Test data 55 shape: (1567, 81)\n",
      "Test data 56 shape: (1567, 81)\n",
      "Test data 57 shape: (1567, 81)\n",
      "Test data 58 shape: (1567, 81)\n",
      "Test data 59 shape: (1567, 81)\n",
      "Test data 60 shape: (1567, 81)\n",
      "Test data 61 shape: (1567, 81)\n",
      "Test data 62 shape: (1567, 81)\n",
      "Test data 63 shape: (1567, 81)\n",
      "Test data 64 shape: (1567, 81)\n",
      "Test data 65 shape: (1567, 81)\n",
      "Test data 66 shape: (1567, 81)\n",
      "Test data 67 shape: (1567, 81)\n",
      "Test data 68 shape: (1567, 81)\n",
      "Test data 69 shape: (1567, 81)\n",
      "Test data 70 shape: (1567, 81)\n",
      "Test data 71 shape: (1567, 81)\n",
      "Test data 72 shape: (1567, 81)\n",
      "Test data 73 shape: (1567, 81)\n",
      "Test data 74 shape: (1567, 81)\n",
      "Test data 75 shape: (1567, 81)\n",
      "Test data 76 shape: (1567, 81)\n",
      "Test data 77 shape: (1567, 81)\n",
      "Test data 78 shape: (1567, 81)\n",
      "Test data 79 shape: (1567, 81)\n",
      "Test data 80 shape: (1567, 81)\n",
      "X_train shape: (673, 1567, 81)\n",
      "y_train shape: (673, 17)\n",
      "X_test shape: (81, 1567, 81)\n",
      "y_test shape: (81, 17)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paths to the directories containing train and test data\n",
    "train_data_path = r'D:\\PHD\\NEW_November2024\\EAWS_npy_final\\separate_npy_per_iterationERGD\\train'\n",
    "test_data_path = r'D:\\PHD\\NEW_November2024\\EAWS_npy_final\\separate_npy_per_iterationERGD\\test'\n",
    "\n",
    "# Function to read .npy files from a given directory\n",
    "def read_npy_files(directory_path):\n",
    "    file_data = {}\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            file_data[file_name] = np.load(file_path)\n",
    "    return file_data\n",
    "\n",
    "# Read train and test data\n",
    "train_data = read_npy_files(train_data_path)\n",
    "#test_data = read_npy_files(test_data_path)\n",
    "test_data_nopad = read_npy_files(test_data_path)\n",
    "\n",
    "\n",
    "# Find the maximum number of frames among all .npy files\n",
    "max_frames = max([data.shape[0] for data in train_data.values()] + [data.shape[0] for data in test_data_nopad.values()])\n",
    "\n",
    "# Pad all .npy files with zeros to match the maximum number of frames\n",
    "def pad_npy_files(data_dict, max_frames):\n",
    "    padded_data = {}\n",
    "    for file_name, data in data_dict.items():\n",
    "        num_frames, num_joints = data.shape\n",
    "        if num_frames < max_frames:\n",
    "            # Pad with zeros to match max_frames\n",
    "            padded_array = np.zeros((max_frames, num_joints))\n",
    "            padded_array[:num_frames, :] = data\n",
    "            padded_data[file_name] = padded_array\n",
    "        else:\n",
    "            padded_data[file_name] = data\n",
    "    return padded_data\n",
    "\n",
    "# Pad train and test data\n",
    "train_data_padded = pad_npy_files(train_data, max_frames)\n",
    "test_data_padded = pad_npy_files(test_data_nopad, max_frames)\n",
    "\n",
    "# Extract labels from file names\n",
    "def extract_labels(file_names):\n",
    "    labels = []\n",
    "    for file_name in file_names:\n",
    "        class_name = file_name.split('_', 2)[-1].rsplit('_', 1)[0]    # Extract class from the file name\n",
    "        print(class_name)\n",
    "        labels.append(class_name)\n",
    "    return labels\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_labels = extract_labels(train_data_padded.keys())\n",
    "test_labels = extract_labels(test_data_padded.keys())\n",
    "\n",
    "# Convert data to lists for easy concatenation\n",
    "train_data_list = list(train_data_padded.values())\n",
    "test_data_list = list(test_data_padded.values())\n",
    "\n",
    "# Check train and test data shapes\n",
    "for i, data in enumerate(train_data_list):\n",
    "    print(f\"Train data {i} shape: {data.shape}\")\n",
    "for i, data in enumerate(test_data_list):\n",
    "    print(f\"Test data {i} shape: {data.shape}\")\n",
    "\n",
    "# Concatenate train and test data\n",
    "#initial correct all_data = train_data_list + test_data_list\n",
    "all_data = train_data_list + test_data_list \n",
    "#initial correct all_labels = train_labels + test_labels\n",
    "all_labels = train_labels + test_labels\n",
    "\n",
    "\n",
    "# Convert to numpy arrays for training\n",
    "X_data = np.array(all_data)  # Shape: [number of iterations, number of frames, number of joints]\n",
    "\n",
    "# Encode labels as categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_data = label_encoder.fit_transform(all_labels)\n",
    "y_data = to_categorical(y_data, num_classes=17)  # Convert labels to categorical format with 17 classes\n",
    "\n",
    "# Split the data back into train and test sets\n",
    "num_train = len(train_data_list)\n",
    "X_train, X_test = X_data[:num_train], X_data[num_train:]\n",
    "y_train, y_test = y_data[:num_train], y_data[num_train:]\n",
    "\n",
    "# Print shapes of prepared data\n",
    "print(\"X_train shape:\", X_train.shape)  # Expected shape: [number of iterations, number of frames, number of joints]\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Data is now ready for training an LSTM network\n",
    "# You can proceed to define your LSTM model and train it using X_train, y_train\n",
    "\n",
    "# # Standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    " # Reshape to 2D for the scaler (flatten frames and joints)\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])).reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])).reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c76bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU, Input, Add\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# # K-Fold Cross-Validation\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# best_val_accuracy = 0\n",
    "# best_train_index, best_val_index = None, None\n",
    "\n",
    "# for train_index, val_index in kf.split(X_data):\n",
    "#     X_train, X_val = X_data[train_index], X_data[val_index]\n",
    "#     y_train, y_val = y_data[train_index], y_data[val_index]\n",
    "\n",
    "#     # Define a CNN Model\n",
    "#     model = Sequential([\n",
    "#         Conv1D(64, kernel_size=3, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         Dropout(0.3),\n",
    "#         Conv1D(128, kernel_size=3, kernel_regularizer=l2(0.01)),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         Dropout(0.3),\n",
    "#         Conv1D(32, kernel_size=3, kernel_regularizer=l2(0.01)),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Flatten(),\n",
    "#         Dense(64, kernel_regularizer=l2(0.01)),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(17, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "#     ])\n",
    "\n",
    "#     # Compile the model\n",
    "#     learning_rate = 0.001\n",
    "#     optimizer = Adam(learning_rate=learning_rate)\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Train the Model\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         epochs=100,\n",
    "#         batch_size=64,\n",
    "#         callbacks=[early_stopping],\n",
    "#         verbose=0\n",
    "#     )\n",
    "\n",
    "#     # Check validation accuracy\n",
    "#     val_accuracy = max(history.history['val_accuracy'])\n",
    "#     if val_accuracy > best_val_accuracy:\n",
    "#         best_val_accuracy = val_accuracy\n",
    "#         best_train_index, best_val_index = train_index, val_index\n",
    "\n",
    "# # Use the best train/validation split for final training\n",
    "# X_train, X_val = X_data[best_train_index], X_data[best_val_index]\n",
    "# y_train, y_val = y_data[best_train_index], y_data[best_val_index]\n",
    "\n",
    "# # Split train data into final train and test sets (use a fixed test set)\n",
    "# X_test = X_data[num_train:]\n",
    "# y_test = y_data[num_train:]\n",
    "\n",
    "# print(f\"Final shapes: Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "# print(f\"Labels shapes: Train: {y_train.shape}, Validation: {y_val.shape}, Test: {y_test.shape}\")\n",
    "\n",
    "# #leave one out\n",
    "# from sklearn.model_selection import LeaveOneOut\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Initialize Leave-One-Out Cross-Validation\n",
    "# loo = LeaveOneOut()\n",
    "# results = []\n",
    "\n",
    "# for train_index, val_index in loo.split(X_train):\n",
    "#     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "#     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "#     # Define your model here\n",
    "#     model = Sequential([\n",
    "#         Conv1D(64, kernel_size=3, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         Dropout(0.3),\n",
    "#         Conv1D(128, kernel_size=3, kernel_regularizer=l2(0.01)),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         Dropout(0.3),\n",
    "#         Conv1D(32, kernel_size=3, kernel_regularizer=l2(0.01)),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Flatten(),\n",
    "#         Dense(64, kernel_regularizer=l2(0.01)),\n",
    "#         LeakyReLU(alpha=0.01),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(17, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "#     ])\n",
    "\n",
    "#     # Compile the model\n",
    "#     optimizer = Adam(learning_rate=0.001)\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Train the model\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "#     history = model.fit(\n",
    "#         X_train_fold, y_train_fold,\n",
    "#         validation_data=(X_val_fold, y_val_fold),\n",
    "#         epochs=100,\n",
    "#         batch_size=64,\n",
    "#         callbacks=[early_stopping],\n",
    "#         verbose=0\n",
    "#     )\n",
    "\n",
    "#     # Evaluate on the validation sample\n",
    "#     val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "#     results.append(val_accuracy)\n",
    "#     print(f\"Validation Accuracy for current fold: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Average validation accuracy across all LOOCV iterations\n",
    "# avg_val_accuracy = np.mean(results)\n",
    "# print(f\"Average Validation Accuracy from LOOCV: {avg_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Train the final model on the entire training set (excluding test data)\n",
    "# print(\"\\nTraining the final model on the entire training set...\")\n",
    "# model = Sequential([\n",
    "#     Conv1D(64, kernel_size=3, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     Conv1D(128, kernel_size=3, kernel_regularizer=l2(0.01)),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     Conv1D(32, kernel_size=3, kernel_regularizer=l2(0.01)),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Flatten(),\n",
    "#     Dense(64, kernel_regularizer=l2(0.01)),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(classes, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "# ])\n",
    "\n",
    "# # Compile and train the final model\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=100,\n",
    "#     batch_size=64,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the final model on the test set\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(f\"Test Data Evaluation: Accuracy = {test_accuracy * 100:.2f}%, Loss = {test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29c0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tensorflow.keras.layers import BatchNormalization, LSTM, Dense, Dropout, Masking\n",
    "# # from tensorflow.keras.models import Sequential\n",
    "# # from tensorflow.keras.callbacks import EarlyStopping\n",
    "# # from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# classes = 17\n",
    "# max_frames = X_train.shape[1]\n",
    "\n",
    "# # Define LSTM Model with Regularization, Batch Normalization, and Learning Rate\n",
    "# print(\"Step 4: Defining the generalized LSTM model with Masking...\")\n",
    "# model = Sequential([\n",
    "#     Masking(mask_value=0.0, input_shape=(max_frames, X_train.shape[2])),  # Ignore padding\n",
    "#     LSTM(256, return_sequences=True, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     LSTM(128, return_sequences=True, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     LSTM(64, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(classes, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "# ])\n",
    "\n",
    "# # Define the optimizer with a learning rate\n",
    "# learning_rate = 0.001\n",
    "# optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# # Train the Model\n",
    "# print(\"Step 5: Training the model with early stopping...\")\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=500,\n",
    "#     batch_size=128,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "# print(\"Training complete!\")\n",
    "\n",
    "# # Evaluate the Model\n",
    "# print(\"Step 6: Evaluating the model on the test data...\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(f\"Test Evaluation: Accuracy = {test_accuracy * 100:.2f}%, Loss = {test_loss:.4f}\")\n",
    "\n",
    "# # Additional: Print Early Stopping Results\n",
    "# if early_stopping.stopped_epoch > 0:\n",
    "#     print(f\"Early stopping triggered after {early_stopping.stopped_epoch} epochs.\")\n",
    "# else:\n",
    "#     print(\"Training ran for the full 500 epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e8cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Defining the generalized CNN model with Masking...\n"
     ]
    }
   ],
   "source": [
    "#with Convolutional Neural Networks\n",
    "#replaced relu with leakyrelu and added an extra layer for the features\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU, Input, Add, Attention\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D  # You can replace with GlobalMaxPooling1D if needed\n",
    "\n",
    "classes = 17\n",
    "max_frames = X_train.shape[1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the training data into new train and validation sets\n",
    "# X_train_new, X_val, y_train_new, y_val = train_test_split(\n",
    "#     X_train, y_train, test_size=0.2, random_state=41, stratify=y_train.argmax(axis=1)\n",
    "# )\n",
    "\n",
    "# Define LSTM Model with Regularization, Batch Normalization, and Learning Rate\n",
    "print(\"Step 4: Defining the generalized CNN model with Masking...\")\n",
    "\n",
    "#this is the one the performs best compared to all \n",
    "model = Sequential([\n",
    "    #Conv1D(64, kernel_size=3, input_shape=(max_frames, X_train.shape[2])),\n",
    "    Conv1D(64, kernel_size=3, kernel_regularizer=l2(0.01), input_shape=(max_frames, X_train.shape[2])),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    #Conv1D(128, kernel_size=3),\n",
    "    Conv1D(128, kernel_size=3, kernel_regularizer=l2(0.01),),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    #Conv1D(32, kernel_size=3),  # Added additional Conv1D layer\n",
    "\n",
    "    Conv1D(32, kernel_size=3, kernel_regularizer=l2(0.01)),  # Added additional Conv1D layer\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Attention layer\n",
    "    #Lambda(lambda x: Attention()([x, x])),\n",
    "    \n",
    "    Flatten(),\n",
    "    # Add GlobalAveragePooling1D to handle variable-length sequences and reduce impact of padding\n",
    "    #GlobalMaxPooling1D(),  # Or GlobalMaxPooling1D()\n",
    "    Dense(64),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# model = Sequential([\n",
    "#     Conv1D(64, kernel_size=3, kernel_regularizer=l2(0.01), input_shape=(max_frames, X_train.shape[2])),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     Conv1D(128, kernel_size=3, kernel_regularizer=l2(0.01)),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     Conv1D(32, kernel_size=3, kernel_regularizer=l2(0.01)),  # Added additional Conv1D layer\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Conv1D(16, kernel_size=3, kernel_regularizer=l2(0.01)),  # Extra Conv1D layer\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Flatten(),\n",
    "#     Dense(64, kernel_regularizer=l2(0.01)),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(classes, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "# ])\n",
    "\n",
    "\n",
    "# #this is to keep orginally\n",
    "# # Define the optimizer with a learning rate\n",
    "# learning_rate = 0.001\n",
    "# optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# # Train the Model\n",
    "# print(\"Step 5: Training the model with early stopping...\")\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=500,\n",
    "#     batch_size=128,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "# print(\"Training complete!\")\n",
    "\n",
    "# # Evaluate the Model\n",
    "# print(\"Step 6: Evaluating the model on the test data...\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(f\"Test Evaluation: Accuracy = {test_accuracy * 100:.2f}%, Loss = {test_loss:.4f}\")\n",
    "\n",
    "# # Additional: Print Early Stopping Results\n",
    "# if early_stopping.stopped_epoch > 0:\n",
    "#     print(f\"Early stopping triggered after {early_stopping.stopped_epoch} epochs.\")\n",
    "# else:\n",
    "#     print(\"Training ran for the full 500 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342d87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from tensorflow.keras.models import clone_model\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU, Input, Add\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# # Number of folds\n",
    "# n_folds = 5\n",
    "# kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# # Placeholder for cross-validation results\n",
    "# cv_accuracies = []\n",
    "# cv_losses = []\n",
    "\n",
    "# # Perform k-fold cross-validation\n",
    "# print(f\"Performing {n_folds}-fold cross-validation...\")\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train.argmax(axis=1))):\n",
    "#     print(f\"Training on fold {fold + 1}/{n_folds}...\")\n",
    "    \n",
    "#     # Split data into train and validation sets for this fold\n",
    "#     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "#     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "#     # Clone the model to ensure weights are reset\n",
    "#     fold_model = clone_model(model)\n",
    "#     fold_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     # Train the model\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=0)\n",
    "#     history = fold_model.fit(\n",
    "#         X_train_fold, y_train_fold,\n",
    "#         validation_data=(X_val_fold, y_val_fold),\n",
    "#         epochs=500,\n",
    "#         batch_size=128,\n",
    "#         callbacks=[early_stopping],\n",
    "#         verbose=0\n",
    "#     )\n",
    "    \n",
    "#     # Evaluate on the validation set\n",
    "#     val_loss, val_accuracy = fold_model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "#     cv_accuracies.append(val_accuracy)\n",
    "#     cv_losses.append(val_loss)\n",
    "#     print(f\"Fold {fold + 1}: Validation Accuracy = {val_accuracy * 100:.2f}%, Loss = {val_loss:.4f}\")\n",
    "\n",
    "# # Cross-validation results\n",
    "# mean_cv_accuracy = np.mean(cv_accuracies)\n",
    "# mean_cv_loss = np.mean(cv_losses)\n",
    "# print(f\"\\nCross-Validation Results: Mean Accuracy = {mean_cv_accuracy * 100:.2f}%, Mean Loss = {mean_cv_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7175cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on the full training set and evaluating on the test set...\n",
      "Epoch 1/500\n",
      "9/9 [==============================] - 4s 82ms/step - loss: 4.6343 - accuracy: 0.2639 - val_loss: 4.5419 - val_accuracy: 0.2370\n",
      "Epoch 2/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 3.2215 - accuracy: 0.6877 - val_loss: 3.5828 - val_accuracy: 0.5630\n",
      "Epoch 3/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 2.7827 - accuracy: 0.7937 - val_loss: 3.1089 - val_accuracy: 0.7259\n",
      "Epoch 4/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 2.4543 - accuracy: 0.9033 - val_loss: 2.9273 - val_accuracy: 0.7407\n",
      "Epoch 5/500\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 2.3075 - accuracy: 0.9312 - val_loss: 2.7646 - val_accuracy: 0.7556\n",
      "Epoch 6/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 2.1358 - accuracy: 0.9647 - val_loss: 2.6739 - val_accuracy: 0.7556\n",
      "Epoch 7/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 2.0601 - accuracy: 0.9796 - val_loss: 2.6137 - val_accuracy: 0.7704\n",
      "Epoch 8/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.9650 - accuracy: 0.9944 - val_loss: 2.5415 - val_accuracy: 0.7556\n",
      "Epoch 9/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.8832 - accuracy: 0.9926 - val_loss: 2.4868 - val_accuracy: 0.7778\n",
      "Epoch 10/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.8187 - accuracy: 0.9926 - val_loss: 2.4210 - val_accuracy: 0.7630\n",
      "Epoch 11/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 1.7497 - accuracy: 0.9981 - val_loss: 2.4076 - val_accuracy: 0.7407\n",
      "Epoch 12/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.6831 - accuracy: 0.9981 - val_loss: 2.3856 - val_accuracy: 0.7185\n",
      "Epoch 13/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.6397 - accuracy: 0.9926 - val_loss: 2.3194 - val_accuracy: 0.7333\n",
      "Epoch 14/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 1.5590 - accuracy: 1.0000 - val_loss: 2.3306 - val_accuracy: 0.7556\n",
      "Epoch 15/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.5099 - accuracy: 0.9963 - val_loss: 2.2508 - val_accuracy: 0.7630\n",
      "Epoch 16/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.4554 - accuracy: 1.0000 - val_loss: 2.1737 - val_accuracy: 0.7481\n",
      "Epoch 17/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.3975 - accuracy: 0.9963 - val_loss: 2.2270 - val_accuracy: 0.7037\n",
      "Epoch 18/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 1.3380 - accuracy: 0.9981 - val_loss: 2.1915 - val_accuracy: 0.6963\n",
      "Epoch 19/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 1.2821 - accuracy: 1.0000 - val_loss: 2.0885 - val_accuracy: 0.7333\n",
      "Epoch 20/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.2292 - accuracy: 1.0000 - val_loss: 2.0481 - val_accuracy: 0.7037\n",
      "Epoch 21/500\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 1.1785 - accuracy: 1.0000 - val_loss: 2.0414 - val_accuracy: 0.7185\n",
      "Epoch 22/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 1.1292 - accuracy: 1.0000 - val_loss: 2.0136 - val_accuracy: 0.6889\n",
      "Epoch 23/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 1.0841 - accuracy: 1.0000 - val_loss: 1.9767 - val_accuracy: 0.6889\n",
      "Epoch 24/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 1.0366 - accuracy: 1.0000 - val_loss: 1.9070 - val_accuracy: 0.7259\n",
      "Epoch 25/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.9911 - accuracy: 0.9981 - val_loss: 1.9322 - val_accuracy: 0.6889\n",
      "Epoch 26/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.9473 - accuracy: 1.0000 - val_loss: 1.9571 - val_accuracy: 0.6444\n",
      "Epoch 27/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.9013 - accuracy: 1.0000 - val_loss: 1.9576 - val_accuracy: 0.6444\n",
      "Epoch 28/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.8604 - accuracy: 1.0000 - val_loss: 1.8710 - val_accuracy: 0.6444\n",
      "Epoch 29/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.8205 - accuracy: 1.0000 - val_loss: 1.7748 - val_accuracy: 0.6593\n",
      "Epoch 30/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.7850 - accuracy: 1.0000 - val_loss: 1.7267 - val_accuracy: 0.7111\n",
      "Epoch 31/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.7675 - accuracy: 0.9963 - val_loss: 1.7463 - val_accuracy: 0.6519\n",
      "Epoch 32/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.7219 - accuracy: 0.9963 - val_loss: 1.7630 - val_accuracy: 0.6074\n",
      "Epoch 33/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.7349 - accuracy: 0.9888 - val_loss: 1.7866 - val_accuracy: 0.6444\n",
      "Epoch 34/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.6686 - accuracy: 0.9907 - val_loss: 1.6434 - val_accuracy: 0.6444\n",
      "Epoch 35/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.6359 - accuracy: 0.9963 - val_loss: 1.5895 - val_accuracy: 0.6667\n",
      "Epoch 36/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.6010 - accuracy: 1.0000 - val_loss: 1.5839 - val_accuracy: 0.7185\n",
      "Epoch 37/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.5727 - accuracy: 1.0000 - val_loss: 1.5102 - val_accuracy: 0.7185\n",
      "Epoch 38/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.5428 - accuracy: 0.9981 - val_loss: 1.5309 - val_accuracy: 0.7037\n",
      "Epoch 39/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.5206 - accuracy: 0.9981 - val_loss: 1.4682 - val_accuracy: 0.6963\n",
      "Epoch 40/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.5000 - accuracy: 0.9944 - val_loss: 1.4464 - val_accuracy: 0.7333\n",
      "Epoch 41/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.4863 - accuracy: 0.9981 - val_loss: 1.4493 - val_accuracy: 0.6815\n",
      "Epoch 42/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4646 - accuracy: 0.9963 - val_loss: 1.2337 - val_accuracy: 0.7778\n",
      "Epoch 43/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4429 - accuracy: 0.9981 - val_loss: 1.1173 - val_accuracy: 0.8222\n",
      "Epoch 44/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4289 - accuracy: 0.9944 - val_loss: 1.0408 - val_accuracy: 0.8222\n",
      "Epoch 45/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.4165 - accuracy: 0.9963 - val_loss: 0.9329 - val_accuracy: 0.8444\n",
      "Epoch 46/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4092 - accuracy: 0.9963 - val_loss: 0.9013 - val_accuracy: 0.8370\n",
      "Epoch 47/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4366 - accuracy: 0.9814 - val_loss: 0.8346 - val_accuracy: 0.8889\n",
      "Epoch 48/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.4469 - accuracy: 0.9777 - val_loss: 0.7916 - val_accuracy: 0.9185\n",
      "Epoch 49/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4422 - accuracy: 0.9721 - val_loss: 0.7727 - val_accuracy: 0.9111\n",
      "Epoch 50/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4641 - accuracy: 0.9665 - val_loss: 0.7836 - val_accuracy: 0.8815\n",
      "Epoch 51/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4566 - accuracy: 0.9703 - val_loss: 0.7353 - val_accuracy: 0.9333\n",
      "Epoch 52/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4227 - accuracy: 0.9796 - val_loss: 0.7006 - val_accuracy: 0.9333\n",
      "Epoch 53/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3940 - accuracy: 0.9907 - val_loss: 0.5118 - val_accuracy: 0.9704\n",
      "Epoch 54/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3504 - accuracy: 0.9981 - val_loss: 0.5760 - val_accuracy: 0.9481\n",
      "Epoch 55/500\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3393 - accuracy: 0.9981 - val_loss: 0.6096 - val_accuracy: 0.9481\n",
      "Epoch 56/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3292 - accuracy: 0.9981 - val_loss: 0.6016 - val_accuracy: 0.9481\n",
      "Epoch 57/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3072 - accuracy: 1.0000 - val_loss: 0.5928 - val_accuracy: 0.9556\n",
      "Epoch 58/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2959 - accuracy: 1.0000 - val_loss: 0.5818 - val_accuracy: 0.9630\n",
      "Epoch 59/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2814 - accuracy: 1.0000 - val_loss: 0.5597 - val_accuracy: 0.9481\n",
      "Epoch 60/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2684 - accuracy: 1.0000 - val_loss: 0.5298 - val_accuracy: 0.9407\n",
      "Epoch 61/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2571 - accuracy: 1.0000 - val_loss: 0.5021 - val_accuracy: 0.9407\n",
      "Epoch 62/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2470 - accuracy: 1.0000 - val_loss: 0.4820 - val_accuracy: 0.9481\n",
      "Epoch 63/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2374 - accuracy: 0.9981 - val_loss: 0.4734 - val_accuracy: 0.9481\n",
      "Epoch 64/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2283 - accuracy: 1.0000 - val_loss: 0.4810 - val_accuracy: 0.9407\n",
      "Epoch 65/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2213 - accuracy: 1.0000 - val_loss: 0.4768 - val_accuracy: 0.9407\n",
      "Epoch 66/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2136 - accuracy: 0.9981 - val_loss: 0.4761 - val_accuracy: 0.9481\n",
      "Epoch 67/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2021 - accuracy: 0.9981 - val_loss: 0.4768 - val_accuracy: 0.9333\n",
      "Epoch 68/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1937 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.9407\n",
      "Epoch 69/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1860 - accuracy: 0.9981 - val_loss: 0.4636 - val_accuracy: 0.9407\n",
      "Epoch 70/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2047 - accuracy: 0.9944 - val_loss: 0.5968 - val_accuracy: 0.8889\n",
      "Epoch 71/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2059 - accuracy: 0.9926 - val_loss: 0.5336 - val_accuracy: 0.9333\n",
      "Epoch 72/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1851 - accuracy: 0.9981 - val_loss: 0.4701 - val_accuracy: 0.9333\n",
      "Epoch 73/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1753 - accuracy: 1.0000 - val_loss: 0.4251 - val_accuracy: 0.9333\n",
      "Epoch 74/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1744 - accuracy: 0.9981 - val_loss: 0.4265 - val_accuracy: 0.9407\n",
      "Epoch 75/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2074 - accuracy: 0.9851 - val_loss: 0.4615 - val_accuracy: 0.9259\n",
      "Epoch 76/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2069 - accuracy: 0.9870 - val_loss: 0.5220 - val_accuracy: 0.9037\n",
      "Epoch 77/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2052 - accuracy: 0.9944 - val_loss: 0.4650 - val_accuracy: 0.9037\n",
      "Epoch 78/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2006 - accuracy: 0.9888 - val_loss: 0.4267 - val_accuracy: 0.9185\n",
      "Epoch 79/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2085 - accuracy: 0.9870 - val_loss: 0.4403 - val_accuracy: 0.9259\n",
      "Epoch 80/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2301 - accuracy: 0.9814 - val_loss: 0.4990 - val_accuracy: 0.9259\n",
      "Epoch 81/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2155 - accuracy: 0.9888 - val_loss: 0.6023 - val_accuracy: 0.9185\n",
      "Epoch 82/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2208 - accuracy: 0.9851 - val_loss: 0.4110 - val_accuracy: 0.9185\n",
      "Epoch 83/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1975 - accuracy: 0.9963 - val_loss: 0.3959 - val_accuracy: 0.9556\n",
      "Epoch 84/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1905 - accuracy: 0.9963 - val_loss: 0.4572 - val_accuracy: 0.9481\n",
      "Epoch 85/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1787 - accuracy: 0.9944 - val_loss: 0.3880 - val_accuracy: 0.9630\n",
      "Epoch 86/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1761 - accuracy: 0.9944 - val_loss: 0.4313 - val_accuracy: 0.9481\n",
      "Epoch 87/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1735 - accuracy: 0.9963 - val_loss: 0.4448 - val_accuracy: 0.9556\n",
      "Epoch 88/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1541 - accuracy: 1.0000 - val_loss: 0.4698 - val_accuracy: 0.9481\n",
      "Epoch 89/500\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1529 - accuracy: 0.9963 - val_loss: 0.4153 - val_accuracy: 0.9556\n",
      "Epoch 90/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1513 - accuracy: 0.9963 - val_loss: 0.4108 - val_accuracy: 0.9704\n",
      "Epoch 91/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1504 - accuracy: 0.9944 - val_loss: 0.4314 - val_accuracy: 0.9481\n",
      "Epoch 92/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2152 - accuracy: 0.9888 - val_loss: 0.3706 - val_accuracy: 0.9630\n",
      "Epoch 93/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1538 - accuracy: 0.9907 - val_loss: 0.2953 - val_accuracy: 0.9407\n",
      "Epoch 94/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1614 - accuracy: 0.9926 - val_loss: 0.6325 - val_accuracy: 0.9407\n",
      "Epoch 95/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1850 - accuracy: 0.9907 - val_loss: 0.4764 - val_accuracy: 0.8963\n",
      "Epoch 96/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2278 - accuracy: 0.9740 - val_loss: 0.3882 - val_accuracy: 0.9185\n",
      "Epoch 97/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1971 - accuracy: 0.9833 - val_loss: 0.4334 - val_accuracy: 0.9259\n",
      "Epoch 98/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1855 - accuracy: 0.9851 - val_loss: 0.3756 - val_accuracy: 0.9333\n",
      "Epoch 99/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1805 - accuracy: 0.9870 - val_loss: 0.3265 - val_accuracy: 0.9259\n",
      "Epoch 100/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1872 - accuracy: 0.9833 - val_loss: 0.3657 - val_accuracy: 0.9333\n",
      "Epoch 101/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1841 - accuracy: 0.9888 - val_loss: 0.3508 - val_accuracy: 0.9407\n",
      "Epoch 102/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1838 - accuracy: 0.9926 - val_loss: 0.3399 - val_accuracy: 0.9630\n",
      "Epoch 103/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1679 - accuracy: 0.9926 - val_loss: 0.3697 - val_accuracy: 0.9556\n",
      "Epoch 104/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1830 - accuracy: 0.9870 - val_loss: 0.4270 - val_accuracy: 0.9407\n",
      "Epoch 105/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1853 - accuracy: 0.9870 - val_loss: 0.5123 - val_accuracy: 0.9037\n",
      "Epoch 106/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1676 - accuracy: 0.9888 - val_loss: 0.3650 - val_accuracy: 0.9556\n",
      "Epoch 107/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1553 - accuracy: 0.9963 - val_loss: 0.6963 - val_accuracy: 0.8815\n",
      "Epoch 108/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1542 - accuracy: 0.9944 - val_loss: 0.4088 - val_accuracy: 0.9407\n",
      "Epoch 109/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1439 - accuracy: 0.9981 - val_loss: 0.3698 - val_accuracy: 0.9481\n",
      "Epoch 110/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1429 - accuracy: 0.9926 - val_loss: 0.3716 - val_accuracy: 0.9481\n",
      "Epoch 111/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1432 - accuracy: 0.9926 - val_loss: 0.3906 - val_accuracy: 0.9333\n",
      "Epoch 112/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1415 - accuracy: 0.9944 - val_loss: 0.3456 - val_accuracy: 0.9259\n",
      "Epoch 113/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1310 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.9037\n",
      "Epoch 114/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1295 - accuracy: 0.9963 - val_loss: 0.3471 - val_accuracy: 0.9333\n",
      "Epoch 115/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1263 - accuracy: 0.9963 - val_loss: 0.3375 - val_accuracy: 0.9407\n",
      "Epoch 116/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1247 - accuracy: 0.9981 - val_loss: 0.3300 - val_accuracy: 0.9407\n",
      "Epoch 117/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1165 - accuracy: 0.9981 - val_loss: 0.3480 - val_accuracy: 0.9556\n",
      "Epoch 118/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1085 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 0.9556\n",
      "Epoch 119/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1046 - accuracy: 1.0000 - val_loss: 0.3210 - val_accuracy: 0.9556\n",
      "Epoch 120/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1001 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 0.9556\n",
      "Epoch 121/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0963 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 0.9481\n",
      "Epoch 122/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0920 - accuracy: 1.0000 - val_loss: 0.3139 - val_accuracy: 0.9481\n",
      "Epoch 123/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.2846 - val_accuracy: 0.9481\n",
      "Epoch 124/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0854 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9481\n",
      "Epoch 125/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0823 - accuracy: 1.0000 - val_loss: 0.2659 - val_accuracy: 0.9407\n",
      "Epoch 126/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0794 - accuracy: 1.0000 - val_loss: 0.3253 - val_accuracy: 0.9259\n",
      "Epoch 127/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0776 - accuracy: 1.0000 - val_loss: 0.3057 - val_accuracy: 0.9407\n",
      "Epoch 128/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0738 - accuracy: 1.0000 - val_loss: 0.2981 - val_accuracy: 0.9481\n",
      "Epoch 129/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 0.2926 - val_accuracy: 0.9481\n",
      "Epoch 130/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0680 - accuracy: 1.0000 - val_loss: 0.2891 - val_accuracy: 0.9407\n",
      "Epoch 131/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.9333\n",
      "Epoch 132/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0632 - accuracy: 1.0000 - val_loss: 0.2971 - val_accuracy: 0.9407\n",
      "Epoch 133/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.2865 - val_accuracy: 0.9481\n",
      "Epoch 134/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0580 - accuracy: 1.0000 - val_loss: 0.2889 - val_accuracy: 0.9481\n",
      "Epoch 135/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9481\n",
      "Epoch 136/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 0.2991 - val_accuracy: 0.9556\n",
      "Epoch 137/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 0.3036 - val_accuracy: 0.9556\n",
      "Epoch 138/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0862 - accuracy: 0.9907 - val_loss: 0.3900 - val_accuracy: 0.9037\n",
      "Epoch 139/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1417 - accuracy: 0.9628 - val_loss: 0.9355 - val_accuracy: 0.8148\n",
      "Epoch 140/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2863 - accuracy: 0.9387 - val_loss: 0.8016 - val_accuracy: 0.8444\n",
      "Epoch 141/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2761 - accuracy: 0.9442 - val_loss: 1.3693 - val_accuracy: 0.8519\n",
      "Epoch 142/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2332 - accuracy: 0.9684 - val_loss: 1.0490 - val_accuracy: 0.9111\n",
      "Epoch 143/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1787 - accuracy: 0.9926 - val_loss: 0.6775 - val_accuracy: 0.9407\n",
      "Epoch 144/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1958 - accuracy: 0.9870 - val_loss: 0.7964 - val_accuracy: 0.9185\n",
      "Epoch 145/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1764 - accuracy: 0.9963 - val_loss: 0.8625 - val_accuracy: 0.8963\n",
      "Epoch 146/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1663 - accuracy: 0.9963 - val_loss: 0.5880 - val_accuracy: 0.9333\n",
      "Epoch 147/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1640 - accuracy: 0.9944 - val_loss: 0.4537 - val_accuracy: 0.9259\n",
      "Epoch 148/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1559 - accuracy: 0.9944 - val_loss: 0.3795 - val_accuracy: 0.9556\n",
      "Epoch 149/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1476 - accuracy: 0.9944 - val_loss: 0.3180 - val_accuracy: 0.9630\n",
      "Epoch 150/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1366 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9556\n",
      "Epoch 151/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1278 - accuracy: 1.0000 - val_loss: 0.3173 - val_accuracy: 0.9556\n",
      "Epoch 152/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1248 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9556\n",
      "Epoch 153/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1176 - accuracy: 1.0000 - val_loss: 0.3119 - val_accuracy: 0.9407\n",
      "Epoch 154/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1131 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.9556\n",
      "\n",
      "Final Test Results: Accuracy = 67.90%, Loss = 1.4768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import clone_model\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU, Input, Add\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Split the training data into new train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=40, stratify=y_train.argmax(axis=1)\n",
    ")\n",
    "\n",
    "# Train on the full training set and evaluate on the test set\n",
    "print(\"\\nTraining on the full training set and evaluating on the test set...\")\n",
    "#final_model = clone_model(model)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=0)\n",
    "\n",
    "\n",
    "final_history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1, \n",
    "    #shuffle=False  # Disable shuffling\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nFinal Test Results: Accuracy = {test_accuracy * 100:.2f}%, Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600705df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### import numpy as np\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Confusion Matrix and Metrics\n",
    "# print(\"Step 7: Generating confusion matrix and metrics...\")\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # Confusion Matrix\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Classification Report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# # Precision, Recall, F1-Score, and Accuracy for each class\n",
    "# precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred_classes, average=None)\n",
    "# accuracy_per_class = []\n",
    "# for i in range(classes):\n",
    "#     class_indices = (y_true == i)\n",
    "#     class_accuracy = (y_pred_classes[class_indices] == y_true[class_indices]).mean()\n",
    "#     accuracy_per_class.append(class_accuracy)\n",
    "#     print(f\"Class {label_encoder.classes_[i]} - Precision: {precision[i]:.2f}, Recall: {recall[i]:.2f}, F1-Score: {fscore[i]:.2f}, Accuracy: {class_accuracy:.2f}, Support: {support[i]}\")\n",
    "\n",
    "# # Plot Training and Validation Loss and Accuracy\n",
    "# print(\"Step 8: Plotting training history...\")\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# # Plot loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(final_history.history['loss'], label='Training Loss')\n",
    "# plt.plot(final_history.history['val_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Loss')\n",
    "\n",
    "# # Plot accuracy\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(final_history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(final_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Define the output directory\n",
    "# output_dir = r\"C:\\Users\\aimove Gavriela\\Desktop\\PHD_test results\\test_7\"\n",
    "# os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# # Confusion Matrix and Metrics\n",
    "# print(\"Step 7: Generating confusion matrix and metrics...\")\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # Confusion Matrix\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.title('Confusion Matrix')\n",
    "\n",
    "# # Save confusion matrix plot\n",
    "# conf_matrix_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "# plt.savefig(conf_matrix_path)\n",
    "# plt.close()\n",
    "# print(f\"Confusion matrix saved to {conf_matrix_path}\")\n",
    "\n",
    "# # Classification Report\n",
    "# print(\"Classification Report:\")\n",
    "# class_report = classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_)\n",
    "# print(class_report)\n",
    "\n",
    "# # Save classification report to a text file\n",
    "# class_report_path = os.path.join(output_dir, \"classification_report.txt\")\n",
    "# with open(class_report_path, \"w\") as f:\n",
    "#     f.write(class_report)\n",
    "# print(f\"Classification report saved to {class_report_path}\")\n",
    "\n",
    "# # Precision, Recall, F1-Score, and Accuracy for each class\n",
    "# precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred_classes, average=None)\n",
    "# accuracy_per_class = []\n",
    "# metrics_path = os.path.join(output_dir, \"per_class_metrics.txt\")\n",
    "\n",
    "# with open(metrics_path, \"w\") as f:\n",
    "#     for i in range(classes):\n",
    "#         class_indices = (y_true == i)\n",
    "#         class_accuracy = (y_pred_classes[class_indices] == y_true[class_indices]).mean()\n",
    "#         accuracy_per_class.append(class_accuracy)\n",
    "#         metrics_line = (\n",
    "#             f\"Class {label_encoder.classes_[i]} - Precision: {precision[i]:.2f}, \"\n",
    "#             f\"Recall: {recall[i]:.2f}, F1-Score: {fscore[i]:.2f}, \"\n",
    "#             f\"Accuracy: {class_accuracy:.2f}, Support: {support[i]}\\n\"\n",
    "#         )\n",
    "#         f.write(metrics_line)\n",
    "#         print(metrics_line.strip())\n",
    "# print(f\"Per-class metrics saved to {metrics_path}\")\n",
    "\n",
    "# # Plot Training and Validation Loss and Accuracy\n",
    "# print(\"Step 8: Plotting training history...\")\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# # Plot loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(final_history.history['loss'], label='Training Loss')\n",
    "# plt.plot(final_history.history['val_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Loss')\n",
    "\n",
    "# # Plot accuracy\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(final_history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(final_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# # Save training history plots\n",
    "# history_plot_path = os.path.join(output_dir, \"training_history.png\")\n",
    "# plt.savefig(history_plot_path)\n",
    "# plt.close()\n",
    "# print(f\"Training history plots saved to {history_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fbb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5f6d1-064f-4010-877c-72e016f167c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
